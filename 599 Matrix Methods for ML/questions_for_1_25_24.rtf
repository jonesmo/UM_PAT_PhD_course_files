{\rtf1\ansi\ansicpg1252\cocoartf2759
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 2.0\
\'93Here w is the weight of the input edge to the neuron.\'94  Edge as in, edges and nodes?\
So x isn\'92t what the AF acts on -- wx + b is?  So each AF acts on a line related to x rather than x itself?\
\
3.0\
So what differentiates a single neuron from multiple neurons in the initial example is that each neuron will use a different weight? But they\'92ll all use the same bias?  So a neuron is just a way to think about a single element of a weight vector?  How do we think about the bias -- is it literally just the y-intercept of the entire function?\
\
3.3\
What are advantages of thinking about the partial derivative of the loss function as a gradient/vector field?\
\
3.4\
What\'92s an intuitive way to understand the gradient of the loss function wrt the weights vector?  So, the gradient of the loss function wrt the weights vector is the direction and magnitude of greatest change of the loss function (as the weight vector changes) in a vector field?  The gradient of L wrt b is the direction and magnitude of the greatest change of L as the bias changes?\
\
3.5\
When I select mu=0.5 or 0.6, the loss drops precipitously after only a few iterations.  Is there something wrong there? (Do we want to slow down the learning process?  How do we select mu to avoid local minima?)\
\
4.2\
How well do I need to know Nesterov\'92s accelerated gradient descent?  Should I know the momentum term used in updating w and b?\
\
5.2\
Could we review my answer to the conceptual question?\
\
7.0\
What\'92s up with the phrase \'93data cube\'94? \
\
7.2\
Why are we looking at accuracy as our first metric for the output of the NN?  What about precision and recall?  Aren\'92t those often more useful?\
\
}