{\rtf1\ansi\ansicpg1252\cocoartf2759
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 With the samples which the network classifies badly, what patterns are we supposed to be picking up?  I noted that either they\'92re trees or that their wave number spectra are significantly different from the \'93average\'94-looking wave number spectrum for the material type.\
\
When we talk about a latent space, we\'92re talking about the span of code representations that are possible for all given input data?  Is that what we mean when we talk about embeddings?  If that\'92s the case, when we talk about \'93word embeddings,\'94 is that something linguists specify or are word embeddings something a network learns and figures out on its own?  Are the feature values encoded in a latent space ever human-interpretable?}