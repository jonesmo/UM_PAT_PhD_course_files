{\rtf1\ansi\ansicpg1252\cocoartf2759
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 5.6.2, the exercise/conceptual question: Is my result for classification accuracy vs. number of digits for different learning rates typical?  Does learning rate not make a significant difference?\
\
5.6.4 What\'92s your take on how the hiddenlayer_output elements could cause the algorithm to fail?\
\
5.6.5 Could we walk through the answer to this?\
\
When training a NN, we\'92re choosing an activation function and optimizing the learning rate, # of steps, and batch_size in addition to the network structure.  Are there any best-practices ways to go about this optimization? It seems scientific to methodically change one of them at a time, but with the resource cost of each round of training, that might not always be practical.\
\
The habit of starting with a learning rate and incrementally decreasing it until the model converges -- is this a habit that\'92s useful for training actual models?  Does it ensure we use only as much compute power as we need?\
\
6.1 What is automatic differentiation?}