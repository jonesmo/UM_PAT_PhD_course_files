{\rtf1\ansi\ansicpg1252\cocoartf2759
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 How can we think about kernels in the context of one-dimensional or time series data like raw audio?\
\
It makes sense to use pooling layers if you\'92re trying to identify images, because it makes the network more resilient to small variations in position.  Does it make sense for audio, where the exact relationships of small features in the audio are important?\
\
Why would we need to flatten the outputs of the convolutional and pooling layers in order to feed them to a dense feedforward network?  Couldn\'92t we keep their shapes as they are at the input?\
\
Is there ever a case where we want to use a different activation function for different kernels within a convolutional layer?\
\
\ul Generation\ulnone \
Thinking about generating raw audio based on classifications.  One strategy is to train a network with raw audio as the input and classifications as the labels.  Another strategy is to use MFCCs, but that would require chaining two networks together: one that generates MFCCs based on labels and another that generates raw audio based on MFCCs.  Are either of these established as a pattern for audio generation?  An autoencoder structure with raw audio as X_train and labels as y_train would be easiest to construct, but it would be more expensive to train a sa whole rather than breaking down the problem into two networks.  Would it be more effective than the other structure?\
\
\ul Generative Network Project Ideas\ulnone \
Soothing sounds based on soothing/alarming sounds dataset\
\
Synthesizer sounds based on AM/FM/subtractive/additive synths dataset\
\
Orchestral chords based on snippets of orchestras with different inversions and orchestrations of chord qualities (Gdim, Bbmaj7, etc.) Or jazz piano voicings!\
\
Feed in a repertoire of Scottish country dance tunes or contradance tunes or other public-domain fiddle tunes and have it generate new tunes -- need a way to encode and capture features of the original tunes\
\
\ul game to play\ulnone :\
Cocoon, Steam\
\
\ul GAN vs Autoencoder\ulnone :\
with an autoencoder, there\'92s no guarantee what\'92s produced is interesting or good\
\
with a GAN, you have a sense that the generator is actually good because you put in normal distribution and get out realistic results (if you\'92ve trained the discriminator well and paid attention to no mode collapse)\
- inside each of these networks, you can do all kinds of network structures\
\
read up on multirate filter banks -- a CNN is a multirate filter bank (aka multitaper systems)\
read up on frequency shift with lower sampling rate / downsampling and replacing with zeroes\
optimizing for nice timbres is not the same as optimizing for the loss function\
	am I optimizing for what really matters? can I quantify that?\
customizing loss functions -- figure out how to do this in Tensorflow\
*loss engineering*\
learn the loss by calibrating sounds and getting people to label them\
weights for different loss terms -- can then create another network to learn the weights / coefficients\
exhaust other tools before loss engineering\
\
- prove to me that _____}